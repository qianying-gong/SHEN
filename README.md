# SHEN: Hardware-Software Co-Exploration of Early-Exit Neural Networks and Edge accelerators

This repository provides the implementation of **SHEN**, which explores the configurations of an Early-Exit(EE) network on customizable edge accelerators. 

Our framework is based on:
- **[Shallow-Deep-Networks](https://github.com/yigitcankaya/Shallow-Deep-Networks)**: Provides the foundation for Early-Exit networks construct and training.
- **[MAESTRO](https://github.com/maestro-project/maestro)**: Provides an analytical cost model evaluating DNN mappings (dataflows and tiling).
## ğŸš€ Overview
SHEN is illustrated below:
<p align="center">
  <img src="framework_overview.png" alt="Framework Overview" style="max-width: 90%; height: auto;">
</p>
Key highlights of our approach:

- SHEN explored the configurations of both DyNNs and DNN accelerators to reduce latency and energy consumption while optimizing accuracy. 
- SHEN proposed a new method to explore the optimal number and positions of ICs for a given backbone model. 
- SHEN used a genetic algorithm to explore the optimal configuration of the accelerator.

Required datasets:
- CIFAR-10, CIFAR-100 (downloaded automatically)
- TinyImageNet

## ğŸ“‚ Repository Structure

â”œâ”€â”€ EarlyExitNetwork/ --- EE networks creating and training\
â”‚   â”œâ”€â”€ architectures/ --- CNN backbones and EE networks\
â”‚   â”œâ”€â”€ w_maestro/ --- transfer EE network to maestro modelfile\
â”‚   â”œâ”€â”€ aux_funcs.py --- auxiliary functions\
â”‚   â”œâ”€â”€ data.py --- Dataset processing scripts \
â”‚   â”œâ”€â”€ early-exit_experiments.py --- evaluate EE networks\
â”‚   â”œâ”€â”€ model_funcs.py --- implements the functions for training and testing EE networks\
â”‚   â”œâ”€â”€ network_architectures.py --- contains the functions to create and save EE networks\
â”‚   â”œâ”€â”€ profiler.py --- compute GFLOPs and num params of EE networks\
â”‚   â””â”€â”€ train_networks.py --- train EE networks via SDN-training strategies\
â”œâ”€â”€ search_algo/ \
â”‚   â”œâ”€â”€ remote_eval/ --- connect meastro and receive latency/energy\
â”‚   â”œâ”€â”€ config.py --- config utilities for yml file\
â”‚   â”œâ”€â”€ config_evo.yml --- project settings\
â”‚   â”œâ”€â”€ eenno_pso_nsga_search.py --- main script\
â”‚   â”œâ”€â”€ utils_eval.py --- utility functions to read/write results\
â”‚   â”œâ”€â”€ utils_opt.py --- functions for the genetic algorithms\
â”‚   â””â”€â”€ utils_pso.py --- functions for adjusting the intermediate classifiers (ICs)\
â”œâ”€â”€ search_space/ \
â”‚   â””â”€â”€ eex_hw_search_space.py --- search space definition\
â””â”€â”€ README.md --- Project documentation \
â””â”€â”€ requirements.yml --- Environment


## ğŸ”§ Installation & Setup
1ï¸âƒ£ Clone the Repository
First, clone this repository to your local machine:
```bash
git clone https://github.com/qianying-gong/SHEN.git
cd SHEN
```
2ï¸âƒ£ Install Dependencies
Make sure you have the required dependencies installed:
```bash
pip install -r requirements.yml
```
3ï¸âƒ£ Configure MAESTRO

This project requires MAESTRO for hardware configuration evaluations. Follow [MAESTRO Website](https://maestro.ece.gatech.edu/). 

4ï¸âƒ£ Configure config_evo.yml

Before running the main script, set up the required parameters in config_evo.yml.

5ï¸âƒ£ Run the Experiment

Once MAESTRO is set up and config_evo.yml is properly configured, execute the main script:
```bash
python3 eenno_pso_nsga_search.py 
```
## ğŸ“ˆ Experimental Results
<p align="center">
  <img src="figure.png" alt="experimental results" style="max-width: 60%; height: auto;">
</p>

#### Figure 2: Comparison of different design points obtained from SHEN, SDN and LoCoExNet for ResNet-56 on CIFAR-10. Latency is measured in cycles and energy is measured in nanojoules (nJ).

---

#### Table 3: Energy-delay product (EDP) Reduction of SHEN compared with the backbone model and the reference approaches
|                              Model                               | CIFAR-10 | CIFAR-100 | TinyImageNet |
|:----------------------------------------------------------------:|:----------:|:----------:|:----------:|
|                           **Backbone**                           | 85.91%   | 62.98%     | 52.35%     |
| **[SDN](https://github.com/yigitcankaya/Shallow-Deep-Networks)** | 10.66%    | 7.53%      | 41.77%     |
|                        **[LoCoExNet](https://ieeexplore.ieee.org/abstract/document/10143348)**                         | 8.98%     | 7.36%      | 27.90%      |

**Key Takeaways:**
- SHEN achieves **higher accuracy with lower EDP** compared to SDN and LoCoExNet.
- Although all methods use the same accelerator, **better DyNN configurations improve efficiency**.
## ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Acknowledgements
We appreciate the contributions of the following projects: 
- [Shallow-Deep-Networks](https://github.com/yigitcankaya/Shallow-Deep-Networks)
- [MAESTRO](https://github.com/maestro-project/maestro)
